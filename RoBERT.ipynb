{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path ='/content/drive/MyDrive/Colab Notebooks/Bengali Dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "texts = data['Text'].tolist()\n",
        "labels = data['Humor'].tolist()\n",
        "\n",
        "label_dict = {label: idx for idx, label in enumerate(set(labels))}\n",
        "numeric_labels = [label_dict[label] for label in labels]\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, numeric_labels, test_size=0.1, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_dict))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53otpy0nIJiT",
        "outputId": "fbc43a81-8908-4029-a4e4-79c1a49bfeb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 64\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "train_accuracy_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_accuracy = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for i in range(0, len(train_encodings['input_ids']), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_input = {key: val[i:i+batch_size].to(device) for key, val in train_encodings.items()}\n",
        "        labels = train_labels[i:i+batch_size].to(device)\n",
        "        outputs = model(**batch_input, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        batch_accuracy = torch.sum(predictions == labels).item() / len(predictions)\n",
        "        total_train_accuracy += batch_accuracy\n",
        "        total_batches += 1\n",
        "\n",
        "    epoch_train_accuracy = total_train_accuracy / total_batches\n",
        "    train_accuracy_values.append(epoch_train_accuracy)\n",
        "    print(f\"Epoch {epoch+1} - Train Accuracy: {epoch_train_accuracy}\")\n",
        "\n",
        "# Test Evaluation\n",
        "model.eval()\n",
        "test_input = {key: val.to(device) for key, val in test_encodings.items()}\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_input)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels, predictions)\n",
        "test_precision = precision_score(test_labels, predictions, average='weighted')\n",
        "test_f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Precision: {test_precision}\")\n",
        "print(f\"Test F1 Score: {test_f1}\")\n"
      ],
      "metadata": {
        "id": "ZHQlXK2YP_2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e220bcf1-fc11-4483-97b1-23e234b405f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Accuracy: 0.4539930555555556\n",
            "Epoch 2 - Train Accuracy: 0.4696180555555556\n",
            "Epoch 3 - Train Accuracy: 0.5512152777777778\n",
            "Epoch 4 - Train Accuracy: 0.556423611111111\n",
            "Epoch 5 - Train Accuracy: 0.5833333333333334\n",
            "Epoch 6 - Train Accuracy: 0.5225694444444444\n",
            "Epoch 7 - Train Accuracy: 0.5303819444444444\n",
            "Epoch 8 - Train Accuracy: 0.5694444444444444\n",
            "Epoch 9 - Train Accuracy: 0.5616319444444444\n",
            "Epoch 10 - Train Accuracy: 0.6276041666666666\n",
            "Epoch 11 - Train Accuracy: 0.6458333333333334\n",
            "Epoch 12 - Train Accuracy: 0.6588541666666666\n",
            "Epoch 13 - Train Accuracy: 0.6432291666666666\n",
            "Epoch 14 - Train Accuracy: 0.6458333333333334\n",
            "Epoch 15 - Train Accuracy: 0.6640625\n",
            "Epoch 16 - Train Accuracy: 0.6484375\n",
            "Epoch 17 - Train Accuracy: 0.6536458333333334\n",
            "Epoch 18 - Train Accuracy: 0.6484375\n",
            "Epoch 19 - Train Accuracy: 0.6744791666666666\n",
            "Epoch 20 - Train Accuracy: 0.6588541666666666\n",
            "Test Accuracy: 0.65\n",
            "Test Precision: 0.6699999999999999\n",
            "Test F1 Score: 0.6446969696969698\n"
          ]
        }
      ]
    }
  ]
}